{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "nVEIH08LA2jr"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8YZeW4jTA563",
    "outputId": "7f186256-a945-4a46-c796-893b85bad824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well! he huffed and he puffed. He puffed and he huffed. And he huffed, huffed, and he puffed, puffed; but he could not blow the house down. At last, he was so out of breath that he couldn't huff and he couldn't puff anymore. So he stopped to rest and thought a bit.\n",
      "But this was too much. The wolf danced about with rage and swore he would come down the chimney and eat up the little pig for his supper. But while he was climbing on to the roof the little pig made up a blazing fire and put on a big pot full of water to boil. Then, just as the wolf was coming down the chimney, the little piggy pulled off the lid, and plop! in fell the wolf into the scalding water.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/content/doc1.txt','r',encoding='utf-16') as file:\n",
    "  text = file.read()\n",
    "  text = str(text)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D_2MckbGpvE"
   },
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3P0dzHTGkmv"
   },
   "source": [
    "**Removing punctuations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "xkAbfZ2vF_2y",
    "outputId": "75373737-03b2-4a35-8f3d-50dbe790d408"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Well he huffed and he puffed He puffed and he huffed And he huffed huffed and he puffed puffed but he could not blow the house down At last he was so out of breath that he couldnt huff and he couldnt puff anymore So he stopped to rest and thought a bit\\nBut this was too much The wolf danced about with rage and swore he would come down the chimney and eat up the little pig for his supper But while he was climbing on to the roof the little pig made up a blazing fire and put on a big pot full of water to boil Then just as the wolf was coming down the chimney the little piggy pulled off the lid and plop in fell the wolf into the scalding water\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punc = string.punctuation\n",
    "for c in text:\n",
    "  if c in punc:\n",
    "    text = text.replace(c,\"\")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jlWltKCwFcdv",
    "outputId": "bc935602-3e50-40bb-86e1-ca0d7cc7a15f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYP27cG5GOfU"
   },
   "source": [
    "**Word Tokenization**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWWI-1iSEtv4",
    "outputId": "9612bbfc-7376-4093-94c8-87bd1d6d72e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Well',\n",
       " 'he',\n",
       " 'huffed',\n",
       " 'and',\n",
       " 'he',\n",
       " 'puffed',\n",
       " 'He',\n",
       " 'puffed',\n",
       " 'and',\n",
       " 'he',\n",
       " 'huffed',\n",
       " 'And',\n",
       " 'he',\n",
       " 'huffed',\n",
       " 'huffed',\n",
       " 'and',\n",
       " 'he',\n",
       " 'puffed',\n",
       " 'puffed',\n",
       " 'but',\n",
       " 'he',\n",
       " 'could',\n",
       " 'not',\n",
       " 'blow',\n",
       " 'the',\n",
       " 'house',\n",
       " 'down',\n",
       " 'At',\n",
       " 'last',\n",
       " 'he',\n",
       " 'was',\n",
       " 'so',\n",
       " 'out',\n",
       " 'of',\n",
       " 'breath',\n",
       " 'that',\n",
       " 'he',\n",
       " 'couldnt',\n",
       " 'huff',\n",
       " 'and',\n",
       " 'he',\n",
       " 'couldnt',\n",
       " 'puff',\n",
       " 'anymore',\n",
       " 'So',\n",
       " 'he',\n",
       " 'stopped',\n",
       " 'to',\n",
       " 'rest',\n",
       " 'and',\n",
       " 'thought',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'But',\n",
       " 'this',\n",
       " 'was',\n",
       " 'too',\n",
       " 'much',\n",
       " 'The',\n",
       " 'wolf',\n",
       " 'danced',\n",
       " 'about',\n",
       " 'with',\n",
       " 'rage',\n",
       " 'and',\n",
       " 'swore',\n",
       " 'he',\n",
       " 'would',\n",
       " 'come',\n",
       " 'down',\n",
       " 'the',\n",
       " 'chimney',\n",
       " 'and',\n",
       " 'eat',\n",
       " 'up',\n",
       " 'the',\n",
       " 'little',\n",
       " 'pig',\n",
       " 'for',\n",
       " 'his',\n",
       " 'supper',\n",
       " 'But',\n",
       " 'while',\n",
       " 'he',\n",
       " 'was',\n",
       " 'climbing',\n",
       " 'on',\n",
       " 'to',\n",
       " 'the',\n",
       " 'roof',\n",
       " 'the',\n",
       " 'little',\n",
       " 'pig',\n",
       " 'made',\n",
       " 'up',\n",
       " 'a',\n",
       " 'blazing',\n",
       " 'fire',\n",
       " 'and',\n",
       " 'put',\n",
       " 'on',\n",
       " 'a',\n",
       " 'big',\n",
       " 'pot',\n",
       " 'full',\n",
       " 'of',\n",
       " 'water',\n",
       " 'to',\n",
       " 'boil',\n",
       " 'Then',\n",
       " 'just',\n",
       " 'as',\n",
       " 'the',\n",
       " 'wolf',\n",
       " 'was',\n",
       " 'coming',\n",
       " 'down',\n",
       " 'the',\n",
       " 'chimney',\n",
       " 'the',\n",
       " 'little',\n",
       " 'piggy',\n",
       " 'pulled',\n",
       " 'off',\n",
       " 'the',\n",
       " 'lid',\n",
       " 'and',\n",
       " 'plop',\n",
       " 'in',\n",
       " 'fell',\n",
       " 'the',\n",
       " 'wolf',\n",
       " 'into',\n",
       " 'the',\n",
       " 'scalding',\n",
       " 'water']"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "words = word_tokenize(text)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7h1LwHAGts2"
   },
   "source": [
    "**Stop Words Removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EE4vSdVWFaDD",
    "outputId": "95b5a72c-af2b-4302-b0ad-adee7f5f90ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fNM5WspbHA8_",
    "outputId": "d56afdb1-454e-442f-e062-42f6cb219397"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Well',\n",
       " 'huffed',\n",
       " 'puffed',\n",
       " 'He',\n",
       " 'puffed',\n",
       " 'huffed',\n",
       " 'And',\n",
       " 'huffed',\n",
       " 'huffed',\n",
       " 'puffed',\n",
       " 'puffed',\n",
       " 'could',\n",
       " 'blow',\n",
       " 'house',\n",
       " 'At',\n",
       " 'last',\n",
       " 'breath',\n",
       " 'couldnt',\n",
       " 'huff',\n",
       " 'couldnt',\n",
       " 'puff',\n",
       " 'anymore',\n",
       " 'So',\n",
       " 'stopped',\n",
       " 'rest',\n",
       " 'thought',\n",
       " 'bit',\n",
       " 'But',\n",
       " 'much',\n",
       " 'The',\n",
       " 'wolf',\n",
       " 'danced',\n",
       " 'rage',\n",
       " 'swore',\n",
       " 'would',\n",
       " 'come',\n",
       " 'chimney',\n",
       " 'eat',\n",
       " 'little',\n",
       " 'pig',\n",
       " 'supper',\n",
       " 'But',\n",
       " 'climbing',\n",
       " 'roof',\n",
       " 'little',\n",
       " 'pig',\n",
       " 'made',\n",
       " 'blazing',\n",
       " 'fire',\n",
       " 'put',\n",
       " 'big',\n",
       " 'pot',\n",
       " 'full',\n",
       " 'water',\n",
       " 'boil',\n",
       " 'Then',\n",
       " 'wolf',\n",
       " 'coming',\n",
       " 'chimney',\n",
       " 'little',\n",
       " 'piggy',\n",
       " 'pulled',\n",
       " 'lid',\n",
       " 'plop',\n",
       " 'fell',\n",
       " 'wolf',\n",
       " 'scalding',\n",
       " 'water']"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_words=[]\n",
    "for word in words:\n",
    "  if word not in stopwords:\n",
    "    clean_words.append(word)\n",
    "\n",
    "words = clean_words\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzK_L9RdHp-F"
   },
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_kiemoZHlcc",
    "outputId": "4cf7b959-a07c-42a7-a1d6-ba12a6107b3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well',\n",
       " 'huf',\n",
       " 'puf',\n",
       " 'He',\n",
       " 'puf',\n",
       " 'huf',\n",
       " 'and',\n",
       " 'huf',\n",
       " 'huf',\n",
       " 'puf',\n",
       " 'puf',\n",
       " 'could',\n",
       " 'blow',\n",
       " 'hous',\n",
       " 'At',\n",
       " 'last',\n",
       " 'breath',\n",
       " 'couldnt',\n",
       " 'huff',\n",
       " 'couldnt',\n",
       " 'puff',\n",
       " 'anymor',\n",
       " 'So',\n",
       " 'stop',\n",
       " 'rest',\n",
       " 'thought',\n",
       " 'bit',\n",
       " 'but',\n",
       " 'much',\n",
       " 'the',\n",
       " 'wolf',\n",
       " 'danc',\n",
       " 'rage',\n",
       " 'swore',\n",
       " 'would',\n",
       " 'come',\n",
       " 'chimney',\n",
       " 'eat',\n",
       " 'littl',\n",
       " 'pig',\n",
       " 'supper',\n",
       " 'but',\n",
       " 'climb',\n",
       " 'roof',\n",
       " 'littl',\n",
       " 'pig',\n",
       " 'made',\n",
       " 'blaze',\n",
       " 'fire',\n",
       " 'put',\n",
       " 'big',\n",
       " 'pot',\n",
       " 'full',\n",
       " 'water',\n",
       " 'boil',\n",
       " 'then',\n",
       " 'wolf',\n",
       " 'come',\n",
       " 'chimney',\n",
       " 'littl',\n",
       " 'piggi',\n",
       " 'pull',\n",
       " 'lid',\n",
       " 'plop',\n",
       " 'fell',\n",
       " 'wolf',\n",
       " 'scald',\n",
       " 'water']"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "stemmed_words = []\n",
    "for word in words:\n",
    "  stemmed_words.append(porter.stem(word))\n",
    "\n",
    "words = stemmed_words\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeuPeVv-Id39"
   },
   "source": [
    "**Removing Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LjUK_mcMIIGm",
    "outputId": "7c0e3518-e37b-420f-a131-0298fff12f58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wolf',\n",
       " 'water',\n",
       " 'chimney',\n",
       " 'plop',\n",
       " 'fire',\n",
       " 'pot',\n",
       " 'hous',\n",
       " 'bit',\n",
       " 'come',\n",
       " 'stop',\n",
       " 'made',\n",
       " 'full',\n",
       " 'He',\n",
       " 'last',\n",
       " 'roof',\n",
       " 'rage',\n",
       " 'anymor',\n",
       " 'fell',\n",
       " 'At',\n",
       " 'danc',\n",
       " 'puff',\n",
       " 'pig',\n",
       " 'littl',\n",
       " 'blaze',\n",
       " 'couldnt',\n",
       " 'big',\n",
       " 'piggi',\n",
       " 'pull',\n",
       " 'blow',\n",
       " 'climb',\n",
       " 'thought',\n",
       " 'then',\n",
       " 'rest',\n",
       " 'puf',\n",
       " 'scald',\n",
       " 'huff',\n",
       " 'much',\n",
       " 'swore',\n",
       " 'and',\n",
       " 'would',\n",
       " 'but',\n",
       " 'well',\n",
       " 'lid',\n",
       " 'boil',\n",
       " 'put',\n",
       " 'eat',\n",
       " 'could',\n",
       " 'supper',\n",
       " 'the',\n",
       " 'huf',\n",
       " 'breath',\n",
       " 'So']"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = list(set(words))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0YS5VSyI81o"
   },
   "source": [
    "**Converting to Lowercase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xDkwAqnIa3R",
    "outputId": "1685b230-83dc-4237-a08c-9d176dd5be0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wolf',\n",
       " 'water',\n",
       " 'chimney',\n",
       " 'plop',\n",
       " 'fire',\n",
       " 'pot',\n",
       " 'hous',\n",
       " 'bit',\n",
       " 'come',\n",
       " 'stop',\n",
       " 'made',\n",
       " 'full',\n",
       " 'he',\n",
       " 'last',\n",
       " 'roof',\n",
       " 'rage',\n",
       " 'anymor',\n",
       " 'fell',\n",
       " 'at',\n",
       " 'danc',\n",
       " 'puff',\n",
       " 'pig',\n",
       " 'littl',\n",
       " 'blaze',\n",
       " 'couldnt',\n",
       " 'big',\n",
       " 'piggi',\n",
       " 'pull',\n",
       " 'blow',\n",
       " 'climb',\n",
       " 'thought',\n",
       " 'then',\n",
       " 'rest',\n",
       " 'puf',\n",
       " 'scald',\n",
       " 'huff',\n",
       " 'much',\n",
       " 'swore',\n",
       " 'and',\n",
       " 'would',\n",
       " 'but',\n",
       " 'well',\n",
       " 'lid',\n",
       " 'boil',\n",
       " 'put',\n",
       " 'eat',\n",
       " 'could',\n",
       " 'supper',\n",
       " 'the',\n",
       " 'huf',\n",
       " 'breath',\n",
       " 'so']"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowercase_words = []\n",
    "for word in words:\n",
    "  lowercase_words.append(word.lower())\n",
    "\n",
    "words = lowercase_words\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4MqhaTWLUKQ"
   },
   "source": [
    "**Adding to vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3pD1fELLTu1",
    "outputId": "76b86c90-8306-476b-edc7-76b51baadcf3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wolf',\n",
       " 'water',\n",
       " 'chimney',\n",
       " 'plop',\n",
       " 'fire',\n",
       " 'pot',\n",
       " 'hous',\n",
       " 'bit',\n",
       " 'come',\n",
       " 'stop',\n",
       " 'made',\n",
       " 'full',\n",
       " 'he',\n",
       " 'last',\n",
       " 'roof',\n",
       " 'rage',\n",
       " 'anymor',\n",
       " 'fell',\n",
       " 'at',\n",
       " 'danc',\n",
       " 'puff',\n",
       " 'pig',\n",
       " 'littl',\n",
       " 'blaze',\n",
       " 'couldnt',\n",
       " 'big',\n",
       " 'piggi',\n",
       " 'pull',\n",
       " 'blow',\n",
       " 'climb',\n",
       " 'thought',\n",
       " 'then',\n",
       " 'rest',\n",
       " 'puf',\n",
       " 'scald',\n",
       " 'huff',\n",
       " 'much',\n",
       " 'swore',\n",
       " 'and',\n",
       " 'would',\n",
       " 'but',\n",
       " 'well',\n",
       " 'lid',\n",
       " 'boil',\n",
       " 'put',\n",
       " 'eat',\n",
       " 'could',\n",
       " 'supper',\n",
       " 'the',\n",
       " 'huf',\n",
       " 'breath',\n",
       " 'so']"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = []\n",
    "vocabulary.extend(words)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lShijhOiQR3-"
   },
   "source": [
    "**Adding to processed docs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "aOymXidAPZks",
    "outputId": "69dbaf5d-89b0-40ff-e74c-d1eb37a511f5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'wolf knew carri scrambl true brick could feast lane hous neither chase insid made hi air away eaten tri appetit jaw greedi slam close larg make almost pig around catch littl three love big fast caught frighten puf smell clamp day noth hadnt door hoov would want blew and two eat huf got the but work so'"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text = ' '.join(map(str, words))\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "024I3mjfPo3w",
    "outputId": "25a4483f-2007-4064-c0e5-5ead6fd2aede"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wolf knew carri scrambl true brick could feast lane hous neither chase insid made hi air away eaten tri appetit jaw greedi slam close larg make almost pig around catch littl three love big fast caught frighten puf smell clamp day noth hadnt door hoov would want blew and two eat huf got the but work so']"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = []\n",
    "processed_docs.append(processed_text)\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RtUmx5BJwu7"
   },
   "source": [
    "## Doing the same for other docs as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caPrBsJeJIxS",
    "outputId": "6206b851-0c60-4b8f-f5c8-ba4b09d3a82f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\t\t\t\t/content/doc2.txt\n",
      "In the sentence above, we can see that there are two \"can\" words, but both of them have different meanings. Here the first \"can\" word is used for question formation. The second \"can\" word at the end of the sentence is used to represent a container that holds food or liquid.\n",
      "\n",
      "Hence, from the examples above, we can see that language processing is not \"deterministic\" (the same language has the same interpretations), and something suitable to one person might not be suitable to another. Therefore, Natural Language Processing (NLP) has a non-deterministic approach. In other words, Natural Language Processing can be used to create a new intelligent system that can understand how humans understand and interpret language in different situations.\n",
      "\n",
      "In the sentence above we can see that there are two can words but both of them have different meanings Here the first can word is used for question formation The second can word at the end of the sentence is used to represent a container that holds food or liquid\n",
      "\n",
      "Hence from the examples above we can see that language processing is not deterministic the same language has the same interpretations and something suitable to one person might not be suitable to another Therefore Natural Language Processing NLP has a nondeterministic approach In other words Natural Language Processing can be used to create a new intelligent system that can understand how humans understand and interpret language in different situations\n",
      "\n",
      "After tokenization: ['In', 'the', 'sentence', 'above', 'we', 'can', 'see', 'that', 'there', 'are', 'two', 'can', 'words', 'but', 'both', 'of', 'them', 'have', 'different', 'meanings', 'Here', 'the', 'first', 'can', 'word', 'is', 'used', 'for', 'question', 'formation', 'The', 'second', 'can', 'word', 'at', 'the', 'end', 'of', 'the', 'sentence', 'is', 'used', 'to', 'represent', 'a', 'container', 'that', 'holds', 'food', 'or', 'liquid', 'Hence', 'from', 'the', 'examples', 'above', 'we', 'can', 'see', 'that', 'language', 'processing', 'is', 'not', 'deterministic', 'the', 'same', 'language', 'has', 'the', 'same', 'interpretations', 'and', 'something', 'suitable', 'to', 'one', 'person', 'might', 'not', 'be', 'suitable', 'to', 'another', 'Therefore', 'Natural', 'Language', 'Processing', 'NLP', 'has', 'a', 'nondeterministic', 'approach', 'In', 'other', 'words', 'Natural', 'Language', 'Processing', 'can', 'be', 'used', 'to', 'create', 'a', 'new', 'intelligent', 'system', 'that', 'can', 'understand', 'how', 'humans', 'understand', 'and', 'interpret', 'language', 'in', 'different', 'situations']\n",
      "After stop words removal: ['In', 'sentence', 'see', 'two', 'words', 'different', 'meanings', 'Here', 'first', 'word', 'used', 'question', 'formation', 'The', 'second', 'word', 'end', 'sentence', 'used', 'represent', 'container', 'holds', 'food', 'liquid', 'Hence', 'examples', 'see', 'language', 'processing', 'deterministic', 'language', 'interpretations', 'something', 'suitable', 'one', 'person', 'might', 'suitable', 'another', 'Therefore', 'Natural', 'Language', 'Processing', 'NLP', 'nondeterministic', 'approach', 'In', 'words', 'Natural', 'Language', 'Processing', 'used', 'create', 'new', 'intelligent', 'system', 'understand', 'humans', 'understand', 'interpret', 'language', 'different', 'situations']\n",
      "After stemming: ['In', 'sentenc', 'see', 'two', 'word', 'differ', 'mean', 'here', 'first', 'word', 'use', 'question', 'format', 'the', 'second', 'word', 'end', 'sentenc', 'use', 'repres', 'contain', 'hold', 'food', 'liquid', 'henc', 'exampl', 'see', 'languag', 'process', 'determinist', 'languag', 'interpret', 'someth', 'suitabl', 'one', 'person', 'might', 'suitabl', 'anoth', 'therefor', 'natur', 'languag', 'process', 'nlp', 'nondeterminist', 'approach', 'In', 'word', 'natur', 'languag', 'process', 'use', 'creat', 'new', 'intellig', 'system', 'understand', 'human', 'understand', 'interpret', 'languag', 'differ', 'situat']\n",
      "After removing duplicates: ['process', 'person', 'second', 'one', 'end', 'word', 'sentenc', 'anoth', 'therefor', 'henc', 'hold', 'question', 'see', 'food', 'use', 'determinist', 'might', 'format', 'system', 'languag', 'nondeterminist', 'In', 'here', 'human', 'nlp', 'mean', 'understand', 'natur', 'exampl', 'new', 'situat', 'first', 'someth', 'repres', 'interpret', 'creat', 'liquid', 'suitabl', 'intellig', 'contain', 'two', 'differ', 'approach', 'the']\n",
      "After converting to lowecase: ['process', 'person', 'second', 'one', 'end', 'word', 'sentenc', 'anoth', 'therefor', 'henc', 'hold', 'question', 'see', 'food', 'use', 'determinist', 'might', 'format', 'system', 'languag', 'nondeterminist', 'in', 'here', 'human', 'nlp', 'mean', 'understand', 'natur', 'exampl', 'new', 'situat', 'first', 'someth', 'repres', 'interpret', 'creat', 'liquid', 'suitabl', 'intellig', 'contain', 'two', 'differ', 'approach', 'the']\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t/content/doc3.txt\n",
      "It uses common sense reasoning for processing tasks. For instance, the freezing temperature can lead to death, or hot coffee can burn people's skin, along with other common sense reasoning tasks. However, this process can take much time, and it requires manual effort.\n",
      "\n",
      "It uses large amounts of data and tries to derive conclusions from it. Statistical NLP uses machine learning algorithms to train NLP models. After successful training on large amounts of data, the trained model will have positive outcomes with deduction.\n",
      "\n",
      "It uses common sense reasoning for processing tasks For instance the freezing temperature can lead to death or hot coffee can burn peoples skin along with other common sense reasoning tasks However this process can take much time and it requires manual effort\n",
      "\n",
      "It uses large amounts of data and tries to derive conclusions from it Statistical NLP uses machine learning algorithms to train NLP models After successful training on large amounts of data the trained model will have positive outcomes with deduction\n",
      "\n",
      "After tokenization: ['It', 'uses', 'common', 'sense', 'reasoning', 'for', 'processing', 'tasks', 'For', 'instance', 'the', 'freezing', 'temperature', 'can', 'lead', 'to', 'death', 'or', 'hot', 'coffee', 'can', 'burn', 'peoples', 'skin', 'along', 'with', 'other', 'common', 'sense', 'reasoning', 'tasks', 'However', 'this', 'process', 'can', 'take', 'much', 'time', 'and', 'it', 'requires', 'manual', 'effort', 'It', 'uses', 'large', 'amounts', 'of', 'data', 'and', 'tries', 'to', 'derive', 'conclusions', 'from', 'it', 'Statistical', 'NLP', 'uses', 'machine', 'learning', 'algorithms', 'to', 'train', 'NLP', 'models', 'After', 'successful', 'training', 'on', 'large', 'amounts', 'of', 'data', 'the', 'trained', 'model', 'will', 'have', 'positive', 'outcomes', 'with', 'deduction']\n",
      "After stop words removal: ['It', 'uses', 'common', 'sense', 'reasoning', 'processing', 'tasks', 'For', 'instance', 'freezing', 'temperature', 'lead', 'death', 'hot', 'coffee', 'burn', 'peoples', 'skin', 'along', 'common', 'sense', 'reasoning', 'tasks', 'However', 'process', 'take', 'much', 'time', 'requires', 'manual', 'effort', 'It', 'uses', 'large', 'amounts', 'data', 'tries', 'derive', 'conclusions', 'Statistical', 'NLP', 'uses', 'machine', 'learning', 'algorithms', 'train', 'NLP', 'models', 'After', 'successful', 'training', 'large', 'amounts', 'data', 'trained', 'model', 'positive', 'outcomes', 'deduction']\n",
      "After stemming: ['It', 'use', 'common', 'sens', 'reason', 'process', 'task', 'for', 'instanc', 'freez', 'temperatur', 'lead', 'death', 'hot', 'coffe', 'burn', 'peopl', 'skin', 'along', 'common', 'sens', 'reason', 'task', 'howev', 'process', 'take', 'much', 'time', 'requir', 'manual', 'effort', 'It', 'use', 'larg', 'amount', 'data', 'tri', 'deriv', 'conclus', 'statist', 'nlp', 'use', 'machin', 'learn', 'algorithm', 'train', 'nlp', 'model', 'after', 'success', 'train', 'larg', 'amount', 'data', 'train', 'model', 'posit', 'outcom', 'deduct']\n",
      "After removing duplicates: ['process', 'amount', 'reason', 'conclus', 'train', 'It', 'common', 'after', 'hot', 'freez', 'for', 'sens', 'use', 'burn', 'peopl', 'along', 'howev', 'manual', 'temperatur', 'algorithm', 'task', 'posit', 'coffe', 'skin', 'requir', 'tri', 'take', 'death', 'nlp', 'larg', 'model', 'deduct', 'instanc', 'outcom', 'lead', 'effort', 'deriv', 'statist', 'learn', 'data', 'much', 'time', 'success', 'machin']\n",
      "After converting to lowecase: ['process', 'amount', 'reason', 'conclus', 'train', 'it', 'common', 'after', 'hot', 'freez', 'for', 'sens', 'use', 'burn', 'peopl', 'along', 'howev', 'manual', 'temperatur', 'algorithm', 'task', 'posit', 'coffe', 'skin', 'requir', 'tri', 'take', 'death', 'nlp', 'larg', 'model', 'deduct', 'instanc', 'outcom', 'lead', 'effort', 'deriv', 'statist', 'learn', 'data', 'much', 'time', 'success', 'machin']\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t/content/doc4.txt\n",
      "Once upon a time there was an old mother pig who had three little pigs and not enough food to feed them. So when they were old enough, she sent them out into the world to seek their fortunes.\n",
      "\n",
      "The first little pig was very lazy. He didn't want to work at all and he built his house out of straw. The second little pig worked a little bit harder but he was somewhat lazy too and he built his house out of sticks. Then, they sang and danced and played together the rest of the day.\n",
      "\n",
      "Once upon a time there was an old mother pig who had three little pigs and not enough food to feed them So when they were old enough she sent them out into the world to seek their fortunes\n",
      "\n",
      "The first little pig was very lazy He didnt want to work at all and he built his house out of straw The second little pig worked a little bit harder but he was somewhat lazy too and he built his house out of sticks Then they sang and danced and played together the rest of the day\n",
      "\n",
      "After tokenization: ['Once', 'upon', 'a', 'time', 'there', 'was', 'an', 'old', 'mother', 'pig', 'who', 'had', 'three', 'little', 'pigs', 'and', 'not', 'enough', 'food', 'to', 'feed', 'them', 'So', 'when', 'they', 'were', 'old', 'enough', 'she', 'sent', 'them', 'out', 'into', 'the', 'world', 'to', 'seek', 'their', 'fortunes', 'The', 'first', 'little', 'pig', 'was', 'very', 'lazy', 'He', 'didnt', 'want', 'to', 'work', 'at', 'all', 'and', 'he', 'built', 'his', 'house', 'out', 'of', 'straw', 'The', 'second', 'little', 'pig', 'worked', 'a', 'little', 'bit', 'harder', 'but', 'he', 'was', 'somewhat', 'lazy', 'too', 'and', 'he', 'built', 'his', 'house', 'out', 'of', 'sticks', 'Then', 'they', 'sang', 'and', 'danced', 'and', 'played', 'together', 'the', 'rest', 'of', 'the', 'day']\n",
      "After stop words removal: ['Once', 'upon', 'time', 'old', 'mother', 'pig', 'three', 'little', 'pigs', 'enough', 'food', 'feed', 'So', 'old', 'enough', 'sent', 'world', 'seek', 'fortunes', 'The', 'first', 'little', 'pig', 'lazy', 'He', 'didnt', 'want', 'work', 'built', 'house', 'straw', 'The', 'second', 'little', 'pig', 'worked', 'little', 'bit', 'harder', 'somewhat', 'lazy', 'built', 'house', 'sticks', 'Then', 'sang', 'danced', 'played', 'together', 'rest', 'day']\n",
      "After stemming: ['onc', 'upon', 'time', 'old', 'mother', 'pig', 'three', 'littl', 'pig', 'enough', 'food', 'feed', 'So', 'old', 'enough', 'sent', 'world', 'seek', 'fortun', 'the', 'first', 'littl', 'pig', 'lazi', 'He', 'didnt', 'want', 'work', 'built', 'hous', 'straw', 'the', 'second', 'littl', 'pig', 'work', 'littl', 'bit', 'harder', 'somewhat', 'lazi', 'built', 'hous', 'stick', 'then', 'sang', 'danc', 'play', 'togeth', 'rest', 'day']\n",
      "After removing duplicates: ['feed', 'second', 'somewhat', 'onc', 'stick', 'straw', 'upon', 'harder', 'food', 'fortun', 'old', 'hous', 'bit', 'built', 'the', 'He', 'lazi', 'didnt', 'pig', 'danc', 'littl', 'three', 'first', 'mother', 'then', 'play', 'rest', 'togeth', 'day', 'seek', 'want', 'sang', 'sent', 'world', 'time', 'enough', 'work', 'So']\n",
      "After converting to lowecase: ['feed', 'second', 'somewhat', 'onc', 'stick', 'straw', 'upon', 'harder', 'food', 'fortun', 'old', 'hous', 'bit', 'built', 'the', 'he', 'lazi', 'didnt', 'pig', 'danc', 'littl', 'three', 'first', 'mother', 'then', 'play', 'rest', 'togeth', 'day', 'seek', 'want', 'sang', 'sent', 'world', 'time', 'enough', 'work', 'so']\n",
      "\n",
      "\n",
      "\n",
      "\t\t\t\t\t\t\t\t\t/content/doc5.txt\n",
      "So he huffed and he puffed and he blew the house down! The wolf was greedy and he tried to catch both pigs at once, but he was too greedy and got neither! His big jaws clamped down on nothing but air and the two little pigs scrambled away as fast as their little hooves would carry them.\n",
      "\n",
      "The wolf chased them down the lane and he almost caught them. But they made it to the brick house and slammed the door closed before the wolf could catch them. The three little pigs they were very frightened, they knew the wolf wanted to eat them. And that was very, very true. The wolf hadn't eaten all day and he had worked up a large appetite chasing the pigs around and now he could smell all three of them inside and he knew that the three little pigs would make a lovely feast.\n",
      "\n",
      "So he huffed and he puffed and he blew the house down The wolf was greedy and he tried to catch both pigs at once but he was too greedy and got neither His big jaws clamped down on nothing but air and the two little pigs scrambled away as fast as their little hooves would carry them\n",
      "\n",
      "The wolf chased them down the lane and he almost caught them But they made it to the brick house and slammed the door closed before the wolf could catch them The three little pigs they were very frightened they knew the wolf wanted to eat them And that was very very true The wolf hadnt eaten all day and he had worked up a large appetite chasing the pigs around and now he could smell all three of them inside and he knew that the three little pigs would make a lovely feast\n",
      "\n",
      "After tokenization: ['So', 'he', 'huffed', 'and', 'he', 'puffed', 'and', 'he', 'blew', 'the', 'house', 'down', 'The', 'wolf', 'was', 'greedy', 'and', 'he', 'tried', 'to', 'catch', 'both', 'pigs', 'at', 'once', 'but', 'he', 'was', 'too', 'greedy', 'and', 'got', 'neither', 'His', 'big', 'jaws', 'clamped', 'down', 'on', 'nothing', 'but', 'air', 'and', 'the', 'two', 'little', 'pigs', 'scrambled', 'away', 'as', 'fast', 'as', 'their', 'little', 'hooves', 'would', 'carry', 'them', 'The', 'wolf', 'chased', 'them', 'down', 'the', 'lane', 'and', 'he', 'almost', 'caught', 'them', 'But', 'they', 'made', 'it', 'to', 'the', 'brick', 'house', 'and', 'slammed', 'the', 'door', 'closed', 'before', 'the', 'wolf', 'could', 'catch', 'them', 'The', 'three', 'little', 'pigs', 'they', 'were', 'very', 'frightened', 'they', 'knew', 'the', 'wolf', 'wanted', 'to', 'eat', 'them', 'And', 'that', 'was', 'very', 'very', 'true', 'The', 'wolf', 'hadnt', 'eaten', 'all', 'day', 'and', 'he', 'had', 'worked', 'up', 'a', 'large', 'appetite', 'chasing', 'the', 'pigs', 'around', 'and', 'now', 'he', 'could', 'smell', 'all', 'three', 'of', 'them', 'inside', 'and', 'he', 'knew', 'that', 'the', 'three', 'little', 'pigs', 'would', 'make', 'a', 'lovely', 'feast']\n",
      "After stop words removal: ['So', 'huffed', 'puffed', 'blew', 'house', 'The', 'wolf', 'greedy', 'tried', 'catch', 'pigs', 'greedy', 'got', 'neither', 'His', 'big', 'jaws', 'clamped', 'nothing', 'air', 'two', 'little', 'pigs', 'scrambled', 'away', 'fast', 'little', 'hooves', 'would', 'carry', 'The', 'wolf', 'chased', 'lane', 'almost', 'caught', 'But', 'made', 'brick', 'house', 'slammed', 'door', 'closed', 'wolf', 'could', 'catch', 'The', 'three', 'little', 'pigs', 'frightened', 'knew', 'wolf', 'wanted', 'eat', 'And', 'true', 'The', 'wolf', 'hadnt', 'eaten', 'day', 'worked', 'large', 'appetite', 'chasing', 'pigs', 'around', 'could', 'smell', 'three', 'inside', 'knew', 'three', 'little', 'pigs', 'would', 'make', 'lovely', 'feast']\n",
      "After stemming: ['So', 'huf', 'puf', 'blew', 'hous', 'the', 'wolf', 'greedi', 'tri', 'catch', 'pig', 'greedi', 'got', 'neither', 'hi', 'big', 'jaw', 'clamp', 'noth', 'air', 'two', 'littl', 'pig', 'scrambl', 'away', 'fast', 'littl', 'hoov', 'would', 'carri', 'the', 'wolf', 'chase', 'lane', 'almost', 'caught', 'but', 'made', 'brick', 'hous', 'slam', 'door', 'close', 'wolf', 'could', 'catch', 'the', 'three', 'littl', 'pig', 'frighten', 'knew', 'wolf', 'want', 'eat', 'and', 'true', 'the', 'wolf', 'hadnt', 'eaten', 'day', 'work', 'larg', 'appetit', 'chase', 'pig', 'around', 'could', 'smell', 'three', 'insid', 'knew', 'three', 'littl', 'pig', 'would', 'make', 'love', 'feast']\n",
      "After removing duplicates: ['wolf', 'knew', 'carri', 'scrambl', 'true', 'brick', 'could', 'feast', 'lane', 'hous', 'neither', 'chase', 'insid', 'made', 'hi', 'air', 'away', 'eaten', 'tri', 'appetit', 'jaw', 'greedi', 'slam', 'close', 'larg', 'make', 'almost', 'pig', 'around', 'catch', 'littl', 'three', 'love', 'big', 'fast', 'caught', 'frighten', 'puf', 'smell', 'clamp', 'day', 'noth', 'hadnt', 'door', 'hoov', 'would', 'want', 'blew', 'and', 'two', 'eat', 'huf', 'got', 'the', 'but', 'work', 'So']\n",
      "After converting to lowecase: ['wolf', 'knew', 'carri', 'scrambl', 'true', 'brick', 'could', 'feast', 'lane', 'hous', 'neither', 'chase', 'insid', 'made', 'hi', 'air', 'away', 'eaten', 'tri', 'appetit', 'jaw', 'greedi', 'slam', 'close', 'larg', 'make', 'almost', 'pig', 'around', 'catch', 'littl', 'three', 'love', 'big', 'fast', 'caught', 'frighten', 'puf', 'smell', 'clamp', 'day', 'noth', 'hadnt', 'door', 'hoov', 'would', 'want', 'blew', 'and', 'two', 'eat', 'huf', 'got', 'the', 'but', 'work', 'so']\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = ['/content/doc2.txt','/content/doc3.txt','/content/doc4.txt','/content/doc5.txt']\n",
    "\n",
    "for doc in docs:\n",
    "\n",
    "  #scanning docs\n",
    "  print(\"\\t\\t\\t\\t\\t\\t\\t\\t\\t\" +doc)\n",
    "  with open(doc,'r',encoding='utf-16') as file:\n",
    "    text = file.read()\n",
    "    text = str(text)\n",
    "    print(text)\n",
    "\n",
    "    # removing punctuations\n",
    "    import string\n",
    "    punc = string.punctuation\n",
    "    for c in text:\n",
    "      if c in punc:\n",
    "        text = text.replace(c,\"\")\n",
    "    print(text)\n",
    "\n",
    "    # tokenization\n",
    "    words = word_tokenize(text)\n",
    "    print(\"After tokenization: {}\".format(words))\n",
    "\n",
    "    # stop words removal\n",
    "    clean_words=[]\n",
    "    for word in words:\n",
    "      if word not in stopwords:\n",
    "        clean_words.append(word)\n",
    "\n",
    "    words = clean_words\n",
    "    print(\"After stop words removal: {}\".format(words))\n",
    "\n",
    "    # stemming\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "      stemmed_words.append(porter.stem(word))\n",
    "\n",
    "    words = stemmed_words\n",
    "    print(\"After stemming: {}\".format(words))\n",
    "\n",
    "    # removing duplicates\n",
    "    words = list(set(words))\n",
    "    print(\"After removing duplicates: {}\".format(words)) \n",
    "\n",
    "    # converting to lowercase\n",
    "    lowercase_words = []\n",
    "    for word in words:\n",
    "      lowercase_words.append(word.lower())\n",
    "\n",
    "    words = lowercase_words\n",
    "    print(\"After converting to lowecase: {}\".format(words))\n",
    "\n",
    "    #adding to vocabulary\n",
    "    vocabulary.extend(words)\n",
    "\n",
    "    #adding to processed docs\n",
    "    processed_text = ' '.join(map(str, words))\n",
    "    processed_docs.append(processed_text)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_As9JJhKTib",
    "outputId": "6052ed74-56bd-459e-ff1c-7c5c5d604637"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['process',\n",
       " 'person',\n",
       " 'wolf',\n",
       " 'amount',\n",
       " 'onc',\n",
       " 'conclus',\n",
       " 'knew',\n",
       " 'one',\n",
       " 'scrambl',\n",
       " 'true']"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = list(set(vocabulary))\n",
    "vocabulary[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSJU58plNEN8"
   },
   "source": [
    "## Creating Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ZhPKwy3LssZ",
    "outputId": "6883da1f-4439-400a-e1c1-bfa4d958899a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wolf knew carri scrambl true brick could feast lane hous neither chase insid made hi air away eaten tri appetit jaw greedi slam close larg make almost pig around catch littl three love big fast caught frighten puf smell clamp day noth hadnt door hoov would want blew and two eat huf got the but work so',\n",
       " 'process person second one end word sentenc anoth therefor henc hold question see food use determinist might format system languag nondeterminist in here human nlp mean understand natur exampl new situat first someth repres interpret creat liquid suitabl intellig contain two differ approach the',\n",
       " 'process amount reason conclus train it common after hot freez for sens use burn peopl along howev manual temperatur algorithm task posit coffe skin requir tri take death nlp larg model deduct instanc outcom lead effort deriv statist learn data much time success machin',\n",
       " 'feed second somewhat onc stick straw upon harder food fortun old hous bit built the he lazi didnt pig danc littl three first mother then play rest togeth day seek want sang sent world time enough work so',\n",
       " 'wolf knew carri scrambl true brick could feast lane hous neither chase insid made hi air away eaten tri appetit jaw greedi slam close larg make almost pig around catch littl three love big fast caught frighten puf smell clamp day noth hadnt door hoov would want blew and two eat huf got the but work so']"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "3K-MAMmWMExP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEVwJfBTQeMH",
    "outputId": "ba0b5df8-cd3e-498b-8334-3e50c6291fd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 1, 1, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 1, 1, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 1, 1, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(processed_docs)\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "MQdtCLvTQ5Gv",
    "outputId": "45187a99-bbbc-4ea2-ed9f-0a177f89b979"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>after</th>\n",
       "      <th>air</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>almost</th>\n",
       "      <th>along</th>\n",
       "      <th>amount</th>\n",
       "      <th>and</th>\n",
       "      <th>anoth</th>\n",
       "      <th>appetit</th>\n",
       "      <th>approach</th>\n",
       "      <th>around</th>\n",
       "      <th>away</th>\n",
       "      <th>big</th>\n",
       "      <th>bit</th>\n",
       "      <th>blew</th>\n",
       "      <th>brick</th>\n",
       "      <th>built</th>\n",
       "      <th>burn</th>\n",
       "      <th>but</th>\n",
       "      <th>carri</th>\n",
       "      <th>catch</th>\n",
       "      <th>caught</th>\n",
       "      <th>chase</th>\n",
       "      <th>clamp</th>\n",
       "      <th>close</th>\n",
       "      <th>coffe</th>\n",
       "      <th>common</th>\n",
       "      <th>conclus</th>\n",
       "      <th>contain</th>\n",
       "      <th>could</th>\n",
       "      <th>creat</th>\n",
       "      <th>danc</th>\n",
       "      <th>data</th>\n",
       "      <th>day</th>\n",
       "      <th>death</th>\n",
       "      <th>deduct</th>\n",
       "      <th>deriv</th>\n",
       "      <th>determinist</th>\n",
       "      <th>didnt</th>\n",
       "      <th>differ</th>\n",
       "      <th>...</th>\n",
       "      <th>see</th>\n",
       "      <th>seek</th>\n",
       "      <th>sens</th>\n",
       "      <th>sent</th>\n",
       "      <th>sentenc</th>\n",
       "      <th>situat</th>\n",
       "      <th>skin</th>\n",
       "      <th>slam</th>\n",
       "      <th>smell</th>\n",
       "      <th>so</th>\n",
       "      <th>someth</th>\n",
       "      <th>somewhat</th>\n",
       "      <th>statist</th>\n",
       "      <th>stick</th>\n",
       "      <th>straw</th>\n",
       "      <th>success</th>\n",
       "      <th>suitabl</th>\n",
       "      <th>system</th>\n",
       "      <th>take</th>\n",
       "      <th>task</th>\n",
       "      <th>temperatur</th>\n",
       "      <th>the</th>\n",
       "      <th>then</th>\n",
       "      <th>therefor</th>\n",
       "      <th>three</th>\n",
       "      <th>time</th>\n",
       "      <th>togeth</th>\n",
       "      <th>train</th>\n",
       "      <th>tri</th>\n",
       "      <th>true</th>\n",
       "      <th>two</th>\n",
       "      <th>understand</th>\n",
       "      <th>upon</th>\n",
       "      <th>use</th>\n",
       "      <th>want</th>\n",
       "      <th>wolf</th>\n",
       "      <th>word</th>\n",
       "      <th>work</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 163 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   after  air  algorithm  almost  along  ...  wolf  word  work  world  would\n",
       "0      0    1          0       1      0  ...     1     0     1      0      1\n",
       "1      0    0          0       0      0  ...     0     1     0      0      0\n",
       "2      1    0          1       0      1  ...     0     0     0      0      0\n",
       "3      0    0          0       0      0  ...     0     0     1      1      0\n",
       "4      0    1          0       1      0  ...     1     0     1      0      1\n",
       "\n",
       "[5 rows x 163 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X.toarray(),columns=vec.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfhv0JkyRIuw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "IRS Prac2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
